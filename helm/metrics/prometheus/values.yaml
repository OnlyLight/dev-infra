replicas: 1

image:
  repository: prom/prometheus
  tag: v2.43.0

configMap:
  name: prometheus-config

prometheus:
  config:
    promethus_yml: |
      global:
        scrape_interval: 15s
      scrape_configs:
      - job_name: 'cadvisor'
        static_configs:
        - targets: ['cadvisor:8080']
      rule_files:
        - "/etc/prometheus/rules.yml"
      alerting:
        alertmanagers:
        - scheme: http
          static_configs:
          - targets:
            - "alertmanager.monitoring.svc:9093"

    rules_yml: |
      groups:
      - name: "high-availability-alert"
        rules:
        - alert: HighDowntime
          expr: kube_pod_status_ready{condition="False"} > 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Pod downtime detected in {{ $labels.namespace }}"
            description: "{{ $labels.pod }} is not ready, affecting HA."

        - alert: HighErrorRate
          expr: rate(http_requests_total{status=~"5xx"}[5m]) > 0.001
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High error rate in {{ $labels.namespace }}"
            description: "Error rate is {{ $value }}, impacting reliability."

      - name: reliability-alerts
        rules:

        - alert: KubernetesNodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Node not ready (instance {{ $labels.instance }})
            description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

        - alert: KubernetesContainerOomKiller
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes Container oom killer (instance {{ $labels.instance }})
            description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"


        - alert: HighLatency
          expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.2 # P95
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "P95 latency >200ms in {{ $labels.namespace }}"
            description: "Latency is {{ $value }}s, check for bottlenecks."

        - alert: HighCPU
          expr: container_cpu_usage_seconds_total{namespace="staging"} > 0.75
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "CPU usage > 75% in {{ $labels.namespace }}"
            description: "CPU usage is {{ $value }}, scale or optimize."

        - alert: ContainerMemoryUsageHigh
          expr: (sum(container_memory_working_set_bytes{namespace=~".*"}) BY (instance, name, pod, namespace) / 
                sum(container_spec_memory_limit_bytes{namespace=~".*"} > 0) BY (instance, name, pod, namespace) * 100) > 80
          for: 5m
          labels:
            severity: critical
          annotations:
            description: '{{ $value | printf "%.2f" }}% High memory usage in namespace {{$labels.namespace }} for  pod {{ $labels.pod }}.'
            resolved_desc: '*RESOLVED* :  Memory usage is normal now (current value is: {{ $value | printf "%.2f"  }})'
            summary: Container Memory Usage is too high.

ports:
  containerPort: 9090
  servicePort: 9090
